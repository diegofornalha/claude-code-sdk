# Suite de Testes - Neo4j Agent API

Suite completa de testes para o projeto Neo4j Agent API com cobertura mÃ­nima de 80%.

## ğŸ“ Estrutura

```
tests/
â”œâ”€â”€ conftest.py              # ConfiguraÃ§Ã£o global e fixtures
â”œâ”€â”€ pytest.ini               # ConfiguraÃ§Ã£o do pytest
â”œâ”€â”€ requirements-test.txt    # DependÃªncias de teste
â”œâ”€â”€ unit/                    # Testes unitÃ¡rios
â”‚   â”œâ”€â”€ test_input_validator.py
â”‚   â”œâ”€â”€ test_claude_handler.py
â”‚   â””â”€â”€ test_session_manager.py
â”œâ”€â”€ integration/             # Testes de integraÃ§Ã£o
â”‚   â”œâ”€â”€ test_api_endpoints.py
â”‚   â”œâ”€â”€ test_chat_flow.py
â”‚   â””â”€â”€ test_websocket.py
â”œâ”€â”€ e2e/                     # Testes end-to-end
â”œâ”€â”€ performance/             # Testes de carga
â”‚   â””â”€â”€ test_load.py
â”œâ”€â”€ mocks/                   # Mocks reutilizÃ¡veis
â”‚   â”œâ”€â”€ mock_claude_sdk.py
â”‚   â””â”€â”€ mock_neo4j.py
â””â”€â”€ fixtures/                # Dados de teste
    â””â”€â”€ sample_data.py
```

## ğŸš€ Executando Testes

### Instalar dependÃªncias
```bash
pip install -r requirements.txt
pip install -r tests/requirements-test.txt
```

### Executar todos os testes
```bash
pytest
```

### Executar por tipo
```bash
# Testes unitÃ¡rios apenas
pytest tests/unit/ -m unit

# Testes de integraÃ§Ã£o
pytest tests/integration/ -m integration

# Testes de performance
pytest tests/performance/ -m performance -m slow
```

### Executar com cobertura
```bash
pytest --cov=core --cov=server --cov-report=html --cov-report=term
```

### Executar testes especÃ­ficos
```bash
# Por arquivo
pytest tests/unit/test_input_validator.py

# Por classe
pytest tests/unit/test_input_validator.py::TestInputValidator

# Por mÃ©todo
pytest tests/unit/test_input_validator.py::TestInputValidator::test_xss_protection
```

### Executar em paralelo
```bash
pytest -n auto  # Usa todos os CPUs disponÃ­veis
pytest -n 4     # Usa 4 workers
```

## ğŸ·ï¸ Marcadores (Markers)

Os testes sÃ£o organizados por marcadores:

- `@pytest.mark.unit` - Testes unitÃ¡rios (funÃ§Ãµes isoladas)
- `@pytest.mark.integration` - Testes de integraÃ§Ã£o
- `@pytest.mark.e2e` - Testes end-to-end
- `@pytest.mark.api` - Testes de endpoints API
- `@pytest.mark.websocket` - Testes de WebSocket
- `@pytest.mark.security` - Testes de seguranÃ§a
- `@pytest.mark.performance` - Testes de performance
- `@pytest.mark.slow` - Testes lentos (>5s)
- `@pytest.mark.neo4j` - Requer Neo4j rodando

### Executar por marcador
```bash
pytest -m unit
pytest -m "integration and not slow"
pytest -m "security"
```

## ğŸ“Š Cobertura de CÃ³digo

Meta: **MÃ­nimo 80% de cobertura**

### Visualizar cobertura
```bash
# Gerar relatÃ³rio
pytest --cov=core --cov=server --cov-report=html

# Abrir no navegador
open htmlcov/index.html  # macOS
xdg-open htmlcov/index.html  # Linux
```

### Cobertura atual
```bash
pytest --cov=core --cov=server --cov-report=term-missing
```

## ğŸ”§ ConfiguraÃ§Ã£o

### pytest.ini
ConfiguraÃ§Ã£o principal do pytest com marcadores e opÃ§Ãµes.

### conftest.py
Fixtures globais compartilhadas:
- `test_client` - Cliente FastAPI de teste
- `async_client` - Cliente async para testes de integraÃ§Ã£o
- `mock_claude_client` - Mock do Claude SDK
- `mock_neo4j_driver` - Mock do Neo4j
- `input_validator` - InstÃ¢ncia do validador
- `claude_handler` - Handler do Claude
- `session_manager` - Gerenciador de sessÃµes

## ğŸ§ª Tipos de Testes

### Unit Tests
Testam componentes isolados com mocks:
- `InputValidator` - ValidaÃ§Ã£o e sanitizaÃ§Ã£o
- `ClaudeHandler` - Gerenciamento de sessÃµes
- `SessionManager` - Rastreamento de sessÃµes

### Integration Tests
Testam fluxos completos da API:
- Endpoints REST
- Streaming SSE
- WebSocket bidirecional
- Rate limiting
- Security headers

### Performance Tests
Medem desempenho e carga:
- RequisiÃ§Ãµes concorrentes
- Tempo de resposta
- Uso de memÃ³ria
- Rate limiter

## ğŸ›¡ï¸ Testes de SeguranÃ§a

Protegem contra:
- XSS (Cross-Site Scripting)
- SQL Injection
- Path Traversal
- Command Injection
- Template Injection

## ğŸ³ Neo4j para Testes

### Docker
```bash
docker run -d \
  --name neo4j-test \
  -p 7474:7474 -p 7687:7687 \
  -e NEO4J_AUTH=neo4j/testpassword \
  neo4j:5.15
```

### Configurar variÃ¡veis
```bash
export NEO4J_URI=bolt://localhost:7687
export NEO4J_USER=neo4j
export NEO4J_PASSWORD=testpassword
```

## ğŸ“ˆ CI/CD

GitHub Actions configurado em `.github/workflows/tests.yml`:

- âœ… Executa em Python 3.10, 3.11, 3.12
- âœ… Neo4j rodando como serviÃ§o
- âœ… Testes unitÃ¡rios + integraÃ§Ã£o
- âœ… Cobertura de cÃ³digo
- âœ… Lint (flake8)
- âœ… Type checking (mypy)
- âœ… Security scan (bandit)
- âœ… Dependency check (safety)
- âœ… Upload de relatÃ³rios

## ğŸ“ Escrevendo Novos Testes

### Template de teste unitÃ¡rio
```python
import pytest
from core.my_module import MyClass

@pytest.mark.unit
class TestMyClass:
    def test_my_function(self):
        # Arrange
        obj = MyClass()

        # Act
        result = obj.my_function()

        # Assert
        assert result == expected_value
```

### Template de teste async
```python
import pytest

@pytest.mark.asyncio
async def test_async_function(async_client):
    response = await async_client.get("/api/endpoint")
    assert response.status_code == 200
```

### Usando mocks
```python
from unittest.mock import Mock, patch

def test_with_mock(mock_claude_client):
    mock_claude_client.query.return_value = "mocked response"

    with patch('core.module.ClaudeClient', return_value=mock_claude_client):
        # Test code
        pass
```

## ğŸ” Debugging Testes

### Executar com output detalhado
```bash
pytest -vv -s
```

### Parar no primeiro erro
```bash
pytest -x
```

### Executar Ãºltimo teste que falhou
```bash
pytest --lf
```

### Debugar com pdb
```bash
pytest --pdb
```

## ğŸ“š Recursos

- [Pytest Documentation](https://docs.pytest.org/)
- [pytest-asyncio](https://pytest-asyncio.readthedocs.io/)
- [Coverage.py](https://coverage.readthedocs.io/)
- [Pytest Best Practices](https://docs.pytest.org/en/stable/goodpractices.html)

## ğŸ¤ Contribuindo

1. Sempre adicione testes para novo cÃ³digo
2. Mantenha cobertura acima de 80%
3. Use marcadores apropriados
4. Documente testes complexos
5. Execute testes localmente antes de commit

---

**Meta de Cobertura**: 80% mÃ­nimo
**Tempo MÃ¡ximo por Teste**: 30s (exceto testes marcados como `slow`)